% Fachstudie NEXUS "Personen im Raum"
% $Id: pir.tex,v 1.15 2005/06/01 08:42:13 garbeam Exp $

\documentclass{article}
\usepackage[german]{babel} 
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{times}
\usepackage[dvips]{graphicx}
\usepackage{url}

\begin{document}

\title{Fachstudie 'Personen im Raum'}
\author{Steffen Keul, Marcel Kilgus, Anselm Garbe\\
\small Universität Stuttgart, Institut für Informatik, Nexus Forschungsprojekt}

\maketitle
\thispagestyle{empty}

\begin{abstract}
In dieser Fachstudie wird die Bewegungserkennung von Personen auf
Bilddaten eines Raumes mithilfe verschiedener Algorithmen aus der
OpenSource Vektor-Grafik Bibliothek OpenCV\cite{opencv} untersucht.
Die Bilddaten werden durch eine handelsübliche Web-Kamera geliefert.
Die Ergebnisse der Bewegungserkennung werden in
Characteristic-Receiver-Operator Curves veranschaulicht. Abschließend
wird die Eignung der untersuchten Algorithmen bewertet.
\end{abstract}

%------------------------------------------------------------------------- 
\section{Einleitung}
Im Rahmen des Nexus Forschungsprojektes, das die Realisierung mobiler
kontext-bezogener Systeme in geeigneten Umgebungsmodellen untersucht,
stellt sich u.a. die Aufgabenstellung durch Bilderkennung von Objekten
spezielle kontext-bezogene Reaktionen in solchen Systeme zu realisieren.
Eine einfache Anwendung wäre beispielsweise die akustische Ausgabe einer
Meldung durch ein System, wenn die letzte Person bei geöffnetem
Fenster einen Raum verläßt, das Fenster zu schließen.
Um eine solche Anwendung realisieren zu können muß zunächst ein
geeigneter Weg zur Bilderkennung gefunden werden.


%------------------------------------------------------------------------- 
\section{Ziel}
Ziel dieser Fachstudie ist die Evaluierung der Eignung von folgenden
Bilderkennungs-Algorithmen aus der OpenCV\cite{opencv} Bibliothek:

\begin{itemize}
\item CalcOpticalFlowHS
\item CalcOpticalFlowLK
\item CalcOpticalFlowBM
\item CalcOpticalFlowPyrLK
\item SegmentMotion
\end{itemize}

Es werden Bilddaten eines Raumes einer handelsüblichen Web-Kamera zur
Untersuchung verwendet. Ein Algorithmus, der zuverlässig Bewegungen
auf Bildsequenzen dieser Bilddaten erkennt, kann für Aussagen zur
Erkennung von Objekten in Betracht gezogen werden.  Personen sind
keine starren Objekte und zeigen Veränderungen in Form von Bewegungen
zwischen einzelnen Bildern einer Bildsequenz auf.  Die Erkennung
solcher Bewegungen kann zu einer Existenzaussage von Personen im
Bildausschnitt approximiert werden. Darüber stellt sich die Frage, in
wie weit sich solche Existenzaussagen zur Zählung von Personen eignen
und welche Bedingungen sich daran knüpfen.

Neben diesen Bewertungen wird ein kleiner Vergleich auf vorhandene
technische Systeme zur Bewegungserkennung gegeben, die auf Bilderkennung
basieren, sowie ein zur untersuchten Vektor-Grafik-Bibliothek
alternatives Produkt.

% Vgl. OpenCV mit anderen Bibliotheken zB MontiVision Development Kit
% OpticalFlow Modul
% evtl. andere Prozesse

%------------------------------------------------------------------------- 
\section{Bilddaten}

Die verwendeten Bilddaten werden mit einer handelsüblichen WebCam,
Modell MOBOTIX M1 (\url{http://www.mobotix.com}), unter identischen
Lichtbedingungen und unverändertem Raumausschnitt erzeugt.  Die
Kameraeinstellungen und Bildeigenschaften sind wie folgt festgelegt:

\begin{figure}[h]
    \begin{center}
        \begin{tabular}{l|l}
        \textbf{Format:} & JPEG\\ \hline
        \textbf{Kompression:} & Standard\\ \hline
        \textbf{Auflösung (Pixel):} & 640x480\\ \hline
        \textbf{Automatischer Lichtfilter:} & aus\\ \hline
        \textbf{Rauschenunterdrückung:} & aus\\ \hline
        \textbf{Hintergrundlicht (-10..10):} & 0\\
        \textbf{Sättigung (-10..10):} & 0\\
        \textbf{Bildschärfe (-2..20):} & 0\\
        \end{tabular}
    {\small \caption{Kameraeinstellungen}}
    \end{center}
\end{figure}

Es werden zur Untersuchung jeweils eine Bildsequenz von 200 Bildern pro
Szenario erstellt.

%------------------------------------------------------------------------- 
\section{Szenarien}
Es werden zwei Szenarien untersucht:

\begin{itemize}
\item keine Person im Bild
\item bewegte Personen im Bild
\end{itemize}

Im ersten Szenario ist keine Person im Bildausschnitt aller 200
Bildsequenzen sichtbar. Im anderen Szenario variiert die Anzahl der
Personen, wie in folgenden Diagrammen, deren Werte auf menschlicher
Wahrnehmung beruhen, veranschaulicht.

\includegraphics[width=7cm,angle=270]{impl/roc-line-no-probands.log.ps}

\includegraphics[width=7cm,angle=270]{impl/roc-line-some-probands.log.ps}

%------------------------------------------------------------------------- 
\section{Algorithmen}

\subsection{Optischer Fluss}

OpenCV bietet drei verschiedene Algorithmen zur Berechnung von
optischem Fluss an.  Diese Algorithmen versuchen die Bewegung von
einem Bild zum nächsten zu charakterisieren.  Ergebnis einer solchen
Charakterisierung sind zwei Matrizen, in denen für jeden Pixel aus dem
ersten Bild die Distanz zu seinem entsprechenden Pixel im zweiten Bild
angegeben wird.  In der ersten Matrix Bewegung in horizontaler, in der
zweiten in vertikaler Richtung.

Diese Matrizen allein geben aber noch keine verwertbare Information,
die auf das Nicht-/Vorhandensein von Personen hindeuten könnte.  Die
Bilder, die von den eingesetzten Kameras geliefert werden, enthalten
ein gewisses Maß an Rauschen, d.h. Unterschiede in der Helligkeit
einzelner Pixel der selben Szene.  Dieses Rauschen führt dazu, dass
die OpenCV-Flussalgorithmen für den überwiegenden Anteil der Pixel
eine Bewegung zwischen zwei Bildern feststellen, obwohl die Bilder
nach Ansicht eines menschlichen Beobachters identisch sind.

Ein weiteres wesentliches Problem ist die Wahl der Parameter der
Fluss-Algorithmen.  Für jeden Pixel aus dem ersten Bild wird innerhalb
einer bestimmten Umgebung nach einer Entsprechung im zweiten Bild
gesucht.  Eine große Umgebung hat zwangsläufig negative Auswirkungen
auf die Rechenzeit des Algorithmus, eine kleine Umgebung hingegen kann
nur sehr lokale Bewegungen feststellen.  Eine Person, die sich durch
den Raum bewegt und von einem Bild zum nächsten eine signifikante
Bewegung durchgeführt hat, wird von einem solchen Verfahren nicht
sinnvoll erkannt werden.

Deshalb scheinen reine Flussalgorithmen ohne weiterreichende Analysen
für die Lösung des Problems ungeeignet.  Die Möglichkeiten von OpenCV
werden hier nur der Vollständigkeit halber angegeben.


\subsubsection{CalcOpticalFlowHS}
Dieser Algorithmus berechnet den optischen Bewegungsfluss für alle Pixel
zwischen zwei Bildern nach dem Horn~\&~Schunk \cite{Horn81} Verfahren.

\subsubsection{CalcOpticalFlowLK}
Dieser Algorithmus berechnet den optischen Bewegungsfluss für alle Pixel
zwischen zwei Bildern nach dem Lucas~\&~Kanade \cite{Lucas81} Verfahren.

\subsubsection{CalcOpticalFlowBM}
Dieser Algorithmus berechnet unterteilt die Eingabebilder in Rechtecke
einer bestimmten Größe, sogenannte Basic Blocks.  Der optische Fluss
wird nun statt für einzelne Pixel für diese Basic Blocks berechnet.
In den Ergebnis-Matrizen steht jeweils ein Eintrag für die Distanz in
Anzahl Pixels, die der entsprechende Basic Block aus dem ersten Bild
sich zu seiner Entsprechung im zweiten Bild bewegt hat.

Für die Personen-Erkennung bleibt das Problem, auf welche Weise die
Größe von Basic Blocks gewählt werden soll, und wie tatsächliche
Bewegungen identifiziert werden können.


\subsection{CalcOpticalFlowPyrLK}
Dieser Algorithmus \cite{Bouget00} basiert auf einer iterativen
Version der optischen Fluss-Pyramiden nach Lucas~\&~Kanade.  Hierbei
werden die beiden Eingabe-Bilder nach einem bestimmten Verfahren
sukzessive um Faktor 4 verkleinert.  So entstehen zwei Pyramiden von
Bildern.  Die untersten Ebenen dieser Pyramiden sind die
Eingabe-Bilder.  Die weiteren Ebenen ihre Verkleinerungen.  Dann wird
mit dem Verfahren nach Lucas~\&~Kanade der optische Fluss zwischen den
einander entsprechenden Ebenen dieser Pyramiden berechnet, beginnend
an der Spitze der Pyramide.  Das Ergebnis einer Ebene wird als
Vermutung für die Berechnung des Flusses in der nächst größeren Ebene
verwendet.  Auf diese Weise kann dieser Algorithmus sowohl sehr lokale
Bewegungen wie auch Bewegungen, die eine Große Strecke des Bildes
betreffen zuverlässige Erkennen.

Somit liefert dieser Algorithmus einen optischen Fluss, der näher an
der gewünschten Information liegt.  Trotzdem besteht noch das gleiche
Problem wie mit den anderen Fluss-Algorithmen, nämlich die Frage, wie
optischer Fluss überhaupt zur Erkennung von Personen eingesetzt werden
kann.

Hierzu liefert OpenCV jedoch eine Antwort, da die Funktion
CalcOpticalFlowPyrLK Funktionalität zum Feature-Tracking mit umfasst.
Der optische Fluss wird verwendet, um die Bewegung einer Menge von
Feature-Points im ersten Bild zu den entsprechenden Positionen im
zweiten Bild zu berechnen.  Feature-Points können durch einen weiteren
Algorithmus automatisch für ein Bild berechnet werden.

Bei automatischer Berechnung der Feature-Points bleibt allerdings
unklar, welche Information sich aus dem Verfolgen dieser Punkte
tatsächlich gewinnen lässt.  Insbesondere ist unklar, wie
sichergestellt wird, dass evtl. im Raum befindliche Personen
tatsächlich von diesen Feature-Points erfasst werden.

In praktischen Tests wurde festgestellt, dass der Algorithmus dazu
neigt, Feature-Points mit der Zeit zu verlieren, dh. im zweiten Bild
gar nicht wieder zu finden.  Insbesondere wenn Objekte durch andere
verdeckt werden, tritt dieser Effekt ein.  Deshalb wird von Zeit zu
Zeit eine Neuberechnung der Feature-Points notwendig.  Durch diese
Neuberechnung geht natürlich Information über die aktuelle Szene
verloren.

Für das Personen im Raum-Problem erschien es sinnvoll, eine feste
Menge von Feature-Points vorzugeben, die an geeigneten Stellen im Bild
strategisch platziert werden.  Jede Person, die den Raum betritt oder
verlässt, wird eine Bewegung der Eingangstür verursachen, genau so wie
Personen, die an einem bestimmten Arbeitsplatz sitzen eben dort
Bewegungen verursachen.



\subsection{SegmentMotion}
Mit Hilfe des SegmentMotion-Algorithmus aus OpenCV lassen sich
Bewegungen die von einem Bild zum nächsten aufgetreten sind, in
einzelne Teilbewegungen zerlegen.  Gehen also zwei Personen in
verschiedenen Teilen des Raumes umher, so soll SegmentMotion die
Bewegungen der beiden Personen in getrennte Bewegungen zweier
verschiedener Objekte unterteilen können.

Durch die Teilung in einzelne Objekte, wird es möglich abschätzungen
über die Heftigkeit einer Bewegung oder die Größe des bewegten Objekts
zu machen.  Beides sind wertvolle Indizien, falls entschieden werden
soll, ob die Detektierung einer Bewegung im Bild durch eine Person
oder nur durch eine Veränderung in den Lichtverhältnissen im Bild
hervorgerufen wurde.

Die OpenCV-Implementierung von SegmentMotion wird angewendet auf eine
Motion~History.  Motion~History ist eine Matrix in der Größe der
Eingabebilder, in der in jeder Zelle ein Zeitstempel eingetragen wird.
Dieser Zeitstempel gibt den letzten Zeitpunkt an, zu dem zuletzte eine
Bewegung dieses Pixels erkannt wurde.  Durch Verwenden der
Motion~History wird es somit möglich, das Verschwinden einer Bewegung
künstlich zu um eine konfigurierbare Zeit zu verzögern.  Dies
ermöglicht auch sehr kurz andauernde, sich aber häufig auftretende
Bewegungen sicher zu erkennen.

Die Einträge in eine Motion~History schließlich werden anhand eines
Differenzbildes erstellt.  Zwei Bilder werden zu Graustufen
umgewandelt und an jedem Koordinatenpunkt wird die Differenz der
Helligkeit der entsprechenden Punkte berechnet.  Ist die Differenz
größer als ein bestimmter Schwellwert, so wird an diesem Punkt eine
Bewegung vermutet.  Die Punkte mit vermuteten Bewegungen werden in die
Motion~History aufgenommen.  Mittels SegmentMotion werden Bewegungen
einzelner Objekte extrahiert und solche Objekte, die extrem klein sind
werden unmittelbar verworfen, da sie mit großer Wahrscheinlichkeit auf
Rauschen zurückzuführen sind.

Ein Vorteil dieses Verfahrens gegenüber dem
CalcOpticalFlowPyrLK-Algorithmus liegt deutlich in der kürzeren
Ausführungszeit.  Auch fällt die Problematik der Feature-Point
Berechnung weg.  Der Algorithmus ist problemlos für alle Räume
anwendbar.  Schwierig ist jedoch die Justierung der Parameter
(Schwellwert für Bewegungserkennung, minimale Objektgröße, maximale
History-Dauer).



\subsection{Eigene Implementierung}
Neben den genannten Algorithmen aus der Vektor-Grafik-Bibliothek
OpenCV\cite{opencv} wurde auch ein eigener Algorithmus entworfen und
untersucht, der unter einer einstellbaren Unschärfeschwelle
Differenzbilder zwischen zwei Bildern erzeugt. Wenn Differenzen
existieren, so werden diese als Bewegungsfelder auf dem Differenzbild
sichtbar.

%------------------------------------------------------------------------- 
\section{Receiver Operator Characteristic Curves}

\includegraphics[width=7cm,angle=270]{impl/roc-optical-flow-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/roc-optical-flow-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/roc-motion-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/roc-motion-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/roc-pyramid-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/roc-pyramid-2.log.ps}

%------------------------------------------------------------------------- 
\section{Empfehlung}
Im Rahmen dieser Fachstudie empfehlen wir die optische Flusserkennung
nach \cite{KIL05}, die auch im Source Code im Anhang vorliegt.

\begin{thebibliography}{99}
\bibitem{opencv} http://opencvlibrary.sf.net
\bibitem{KIL05} Marcel Kilgus, Optische Flusserkennung mit 15 LOC, 2005.
\bibitem{Horn81} Berthold K. P. Horn und Brian G. Schunck. Determining
Optical Flow. Artificial Intelligence, 17, pp. 185-203, 1981.
\bibitem{Lucas81} Lucas, B. und Kanade, T. An Iterative Image
Registration Technique with an Application to Stereo Vision, Proc. of
7th International Joint Conference on Artificial Intelligence (IJCAI),
pp. 674-679.
\bibitem{Bouget00} Jean-Yves Bouguet. Pyramidal Implementation of the
Lucas Kanade Feature Tracker.
\end{thebibliography}

\section*{Anhang}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d10.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d10m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d10m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d1m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d1m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d2m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d2m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d3m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d3m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d5.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d5m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-1-d5m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d10.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d10m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d10m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d1m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d1m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d2m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d2m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d3m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d3m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d5.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d5m001M1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/motion-2-d5m001M3.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-f1-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-f1-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-f2-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t05-f1-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t05-f1-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t05-f2-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t05-f2-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t100-f1-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t100-f1-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t100-f2-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t100-f2-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t50-f1-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t50-f1-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t50-f2-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/pyramid-t50-f2-2.log.ps}

\includegraphics[width=7cm,angle=270]{impl/test-1.log.ps}

\includegraphics[width=7cm,angle=270]{impl/test-2.log.ps}

\end{document}
